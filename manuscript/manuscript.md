# MAGICC: ultra-fast genome quality assessment using core gene k-mer profiles and deep learning

## Abstract

Metagenome-assembled genomes (MAGs) and single-cell amplified genomes (SAGs) are central to culture-independent microbiology, yet quality assessment remains a critical bottleneck. Existing tools such as CheckM2, CoCoPyE, and DeepCheck achieve reasonable completeness estimation but severely underestimate contamination—particularly when contaminant sequences originate from phylogenetically distant organisms. Using NCBI-finished reference genomes with known ground truth, we show that at true contamination levels above 20%, these tools produce mean absolute errors (MAEs) of 26–39%, rendering contamination estimates unreliable for real-world MAGs and SAGs. Here we present MAGICC (Metagenome-Assembled Genome Inference of Completeness and Contamination), which replaces gene annotation with core gene k-mer profiling to capture the compositional shift that foreign DNA introduces. MAGICC combines 9,249 canonical 9-mer counts derived from bacterial and archaeal single-copy core genes with 26 assembly statistics in a dual-branch neural network featuring squeeze-and-excitation attention and cross-attention fusion. Trained on one million synthetic genomes spanning 110 phyla, MAGICC achieves 2.74% completeness MAE and 3.60% contamination MAE across five benchmark sets—a 6.4-fold improvement in contamination accuracy over CheckM2. MAGICC processes genomes at 1,451 genomes per minute per thread (~1,700× faster than CheckM2), requires only 0.66 GB of memory (28× less than CheckM2), and is available as a single-command Python tool and web server.

## Introduction

The reconstruction of genomes from metagenomic sequencing data has transformed microbiology, enabling the discovery of thousands of previously unknown bacterial and archaeal lineages^1,2^. Metagenome-assembled genomes (MAGs) and single-cell amplified genomes (SAGs) are central to culture-independent microbiology, yet both genome types require rigorous quality assessment. MAGs are generated through computational binning of assembled contigs, a process that is inherently imperfect, while SAGs obtained through single-cell genomics suffer from amplification biases that lead to incomplete and uneven genome recovery. The resulting genomes exhibit varying degrees of completeness—the fraction of the true genome that was recovered—and contamination—the inclusion of sequences from other organisms. Accurate quality assessment is essential before downstream analyses, and the Minimum Information about a Metagenome-Assembled Genome (MIMAG) standards classify MAGs into high-quality (≥90% complete, <5% contaminated), medium-quality (≥50% complete, <10% contaminated), and low-quality categories^3^.

The current standard for MAG quality assessment is CheckM2^4^, which predicts completeness and contamination using gradient boosted decision trees and neural networks trained on KEGG ortholog (KO) annotation features. CheckM2 represented a major advance over its predecessor CheckM^5^, which relied on lineage-specific marker gene sets and phylogenetic placement. However, CheckM2's pipeline requires gene prediction with Prodigal^6^, protein alignment with DIAMOND^7^ against the UniRef100 database, and subsequent KO annotation—a computationally expensive process that yields approximately 0.8 genomes per minute per thread and demands ~19 GB of peak memory. For the rapidly growing repositories of MAGs—the gcMeta database now contains over 2.7 million MAGs^8^—this translates to processing times measured in months rather than hours.

More fundamentally, CheckM2's contamination model has an intrinsic blind spot rooted in its training data. During training, contamination was simulated exclusively through self-contamination — randomly resampling proteins from the same genome to generate contamination levels of 0–35% — meaning that contamination was operationally defined as the duplication of a genome's own protein content^4^. Cross-contamination from divergent organisms was used only for benchmarking, not for model training, and was limited to a maximum of 10%. Consequently, the gradient boosted model learned to associate contamination with the functional signature of duplicated self-content: inflated counts of KO annotations already encoded in the host genome. When foreign DNA from a different lineage is present, the contaminant introduces a distinct set of KO annotations rather than duplicating existing ones, producing a signal fundamentally different from what the model was trained to detect. Unlike CheckM1's marker-gene approach — which infers contamination through duplicated single-copy genes and thus at least detects redundancy regardless of source — CheckM2's functional annotation features lack an inherent mechanism for identifying non-self sequences; they capture what functions are encoded, not which organism they came from. When contaminant DNA from a divergent lineage introduces novel KO annotations absent from the host genome, the model may misinterpret the expanded functional repertoire as increased completeness rather than contamination, since this signal bears no resemblance to the duplicated self-content on which the contamination model was trained. Indeed, while CheckM2 outperformed CheckM1 for detecting cross-contamination in high-quality genomes, its accuracy declined notably for medium-quality genomes contaminated by higher-taxa sources^4^. The authors themselves acknowledged this limitation, noting that "detection of contamination from divergent taxonomic sources may be improved through more complex training data simulation."

Other tools face similar limitations. CoCoPyE^9^ uses protein domain profiles (Pfam) for feature extraction, inheriting the same functional-annotation blind spot. DeepCheck^10^ applies a ResNet architecture to CheckM2's intermediate feature vectors, remaining dependent on CheckM2's annotation pipeline and thus inheriting both its computational cost and its feature limitations. GUNC^11^ takes a fundamentally different approach by analyzing the taxonomic consistency of individual contigs, but operates as a contamination detector (binary flag) rather than providing quantitative estimates.

To quantify this gap, we evaluated CheckM2, CoCoPyE, and DeepCheck on synthetic genomes derived exclusively from NCBI-finished reference genomes, where the true completeness and contamination are known by construction. While all three tools achieved reasonable completeness prediction (MAE 2.5–4.2%), their contamination estimates deteriorated dramatically with increasing contamination levels: at true contamination above 20%, MAEs ranged from 26% to 39%. This systematic underestimation means that a genome with 60% true contamination might be reported as having only 20–30%—a difference that could fundamentally alter downstream biological conclusions.

We reasoned that k-mer composition of single-copy core genes could address this gap. Genomic k-mer profiles carry rich taxonomic signal—a principle exploited by metagenomic binning tools such as MetaBAT2^12^ and CONCOCT^13^, which cluster contigs by tetranucleotide frequency. When foreign DNA is introduced into a genome, the k-mer profile shifts measurably because different organisms have distinct nucleotide composition patterns shaped by GC content, codon usage bias, and evolutionary divergence. Critically, k-mer counting requires only a single pass through the sequence data, without gene prediction, alignment, or database searches.

Here we present MAGICC (Metagenome-Assembled Genome Inference of Completeness and Contamination), which combines core gene k-mer profiles with assembly statistics in a dual-branch neural network to predict genome quality. By selecting 9,249 canonical 9-mers from bacterial and archaeal core genes and pairing them with 26 assembly statistics that capture structural signatures of contamination (e.g., GC bimodality, GC outlier fraction), MAGICC achieves accurate predictions across the full range of completeness (50–100%) and contamination (0–100%), processes genomes three orders of magnitude faster than existing tools, and requires minimal computational resources.

## Results

### Existing tools systematically underestimate contamination from foreign organisms

To establish the need for a new approach, we first benchmarked three current tools—CheckM2 v1.0.1, CoCoPyE v0.5.0, and DeepCheck—on controlled synthetic genomes derived from 1,810 NCBI-finished reference genomes (assembly level "Complete Genome" or "Chromosome") from 35 phyla (**Fig. 1**; **Table S1**). Using finished genomes as the ground truth eliminates any circularity in quality assessment, as these genomes have independently verified assemblies.

In a completeness gradient experiment (Set A: 1,000 genomes, 0% contamination, completeness spanning 50–100%), all tools performed reasonably: CheckM2 achieved 2.54% completeness MAE, CoCoPyE 3.47%, and DeepCheck 4.15% (**Fig. 1b**). However, in a contamination gradient experiment (Set B: 1,000 genomes, 100% complete, contamination spanning 0–80% from cross-phylum sources), performance collapsed: CheckM2's contamination MAE rose to 17.08%, CoCoPyE to 19.14%, and DeepCheck to 25.70% (**Fig. 1c**), with predicted contamination falling far below the true values (**Fig. 1d**). At contamination levels above 20%, the tools produced MAEs of 25.8%, 30.6%, and 38.6%, respectively. In a realistic mixed set (Set C: 1,000 genomes combining varied completeness and contamination levels), contamination MAE remained high across all tools (18.1–24.4%) (**Fig. 1c,e**).

Notably, the presence of foreign contamination also confuses completeness estimation. When partial genomes are mixed with foreign sequences (Set C), completeness MAEs increase significantly compared to sets with only completeness variation (Set A) or only contamination variation (Set B). In Set A (pure completeness variation), completeness MAEs were modest: CheckM2 2.54%, CoCoPyE 3.47%, DeepCheck 4.15%. In Set B (contamination only, no incompleteness), completeness MAEs were even lower: CheckM2 0.42%, CoCoPyE 2.57%, DeepCheck 6.74%. However, in Set C (realistic genomes with both incomplete and contaminated genomes), completeness MAEs rose sharply: CheckM2 9.13%, CoCoPyE 4.99%, DeepCheck 11.96% (**Fig. 1b**). This demonstrates that foreign contamination confuses these tools' completeness estimation, likely because contaminant sequences inflate the apparent gene content, causing the models to overestimate how complete a genome is.

These empirical results directly trace to three design choices in CheckM2's contamination model^4^: (i) contamination was simulated exclusively through self-contamination at 0–35%, meaning the model never encountered foreign sequences during training; (ii) cross-contamination from divergent organisms was used only for benchmarking, not training, and was limited to 0–10%, far below the contamination levels prevalent in real metagenomic data; and (iii) contamination was operationally defined as duplication of a genome's own protein content—the model detects inflated counts of KO annotations already encoded in the host genome, but contaminants from distant lineages introduce entirely novel proteins with no orthologs in the host, producing a signal invisible to the contamination detector. CoCoPyE and DeepCheck inherit analogous blind spots through their reliance on functional annotation features.

The speed of existing tools further limits their applicability. All three tools processed fewer than 1 genome per minute per thread, meaning that assessing 100,000 genomes—a routine scale for modern metagenomic studies^8^—would require weeks of computation on standard hardware.

### MAGICC overview

MAGICC takes a FASTA genome file as input and produces completeness and contamination predictions through four steps (**Fig. 2**). First, the genome sequence is scanned in a single pass using a Numba-accelerated rolling hash to count 9,249 pre-selected canonical 9-mers derived from bacterial core genes (85 single-copy marker families) and archaeal core genes (128 marker families). These k-mers were selected by prevalence across 1,000 representative bacterial and 1,000 representative archaeal genomes from the GTDB^14^ training set, capturing k-mers present in the broadest range of organisms. Second, 26 assembly statistics are computed, including 11 contig length metrics (N50, L50, contig count, etc.), 4 GC composition features (mean, standard deviation, interquartile range, bimodality index), 4 distributional features (GC outlier fraction, largest contig fraction, top-10% concentration, N50/mean ratio), 1 total k-mer count feature, and 6 k-mer summary statistics (unique k-mer count, duplicate ratio, Shannon entropy, etc.). Third, features are normalized using pre-computed statistics (Z-score for k-mers, log-transform for lengths, min-max for proportions, robust scaling for counts). Fourth, the normalized features are passed through a dual-branch neural network: the k-mer branch (9,249→4,096→1,024→256 with squeeze-and-excitation attention blocks after each dense layer) and the assembly branch (26→128→64) are fused via cross-attention (assembly embedding queries k-mer tokens) into a final prediction head (320→128→64→2), outputting completeness [50–100%] and contamination [0–100%].

The model was trained on one million synthetic genomes generated from 100,000 high-quality reference genomes (≥98% CheckM2 completeness, ≤2% contamination, <100 contigs, N50 >20 kbp) spanning 110 phyla from the GTDB. Training genomes were synthesized with realistic fragmentation patterns (four quality tiers with coverage dropout, GC-biased loss, and repeat exclusion), contamination from within-phylum (1–3 genomes) and cross-phylum (1–5 genomes) sources distributed via Dirichlet allocation, and completeness ranging from 50% to 100%. The model was trained with mixed-precision FP16, AdamW optimizer (learning rate 1×10⁻³, weight decay 5×10⁻⁴), cosine annealing warm restarts, and a weighted MSE loss (2× completeness weight) for 84 epochs with early stopping.

### MAGICC achieves accurate predictions across diverse benchmarks

We evaluated MAGICC on five benchmark sets totaling 5,000 genomes, all derived from finished reference genomes not used in training (**Fig. 3**, **Table 1**). Set A (1,000 genomes, completeness gradient, 0% contamination) tested completeness prediction in isolation: MAGICC achieved 1.99% MAE, comparable to CheckM2 (2.54%) (**Fig. 3b**). Set B (1,000 genomes, 100% complete, contamination gradient 0–80%) tested contamination prediction: MAGICC achieved 2.78% MAE, a dramatic improvement over CheckM2 (17.66%), CoCoPyE (19.14%), and DeepCheck (25.70%) (**Fig. 3c,d**). Set C (1,000 Patescibacteria genomes) and Set D (1,000 Archaea genomes) tested challenging lineages with reduced genomes and underrepresented taxa: MAGICC maintained strong performance (Set C: 2.96%/5.05% completeness/contamination MAE; Set D: 4.06%/5.40%), while competitor tools produced contamination MAEs of 26–45% (**Fig. 3e,f**). Set E (1,000 mixed genomes with realistic composition) confirmed MAGICC's robustness (3.92%/4.32% MAE) (**Fig. 3g**).

Across all five benchmark sets, MAGICC achieved overall completeness MAE of 2.74% (R²=0.90) and contamination MAE of 3.60% (R²=0.96), compared to CheckM2's 6.00%/23.14%, CoCoPyE's 6.62%/20.26%, and DeepCheck's 10.10%/27.57% (**Table 1**; per-set details in **Table S2**). The improvement was most striking for contamination, where MAGICC's MAE was 6.4× lower than CheckM2's. All improvements were statistically significant (paired Wilcoxon signed-rank test, p<0.001; 1,000-replicate bootstrap 95% confidence intervals non-overlapping). Per-set predicted versus true quality scores for all tools are shown in **Supplementary Figures S1--S5**.

**Table 1. Benchmark results across five evaluation sets (5,000 genomes total).**

| Tool | Comp. MAE (%) | Cont. MAE (%) | Comp. R² | Cont. R² | Speed (G/min/thread) | Peak memory (GB) |
|------|:---:|:---:|:---:|:---:|:---:|:---:|
| **MAGICC** | **2.74** | **3.60** | **0.90** | **0.96** | **1,451** | **0.66** |
| CheckM2 | 6.00 | 23.14 | 0.67 | 0.34 | 0.82 | 18.76 |
| CoCoPyE | 6.62 | 20.26 | 0.65 | 0.65 | 0.70 | 15.93 |
| DeepCheck | 10.10 | 27.57 | 0.48 | 0.18 | 0.82* | 1.46† |

*DeepCheck requires CheckM2 feature extraction; effective speed equals CheckM2.
†Inference-only memory; requires CheckM2 pipeline for feature extraction.

For MIMAG quality classification on Sets C and D (**Table S3**), MAGICC achieved macro-averaged F1 scores of 0.89, compared to 0.28–0.37 for CheckM2, 0.42–0.71 for CoCoPyE, and 0.21–0.27 for DeepCheck. This demonstrates that MAGICC's improved contamination estimation translates directly to more reliable quality tier assignments.

### MAGICC is computationally efficient

Beyond accuracy, MAGICC offers dramatic computational advantages (**Table S4**). By eliminating gene prediction, protein alignment, and database search, MAGICC processes genomes at 1,451 genomes per minute per thread—approximately 1,700× faster than CheckM2 (0.82 genomes/min/thread) and 2,100× faster than CoCoPyE (0.70 genomes/min/thread). This speed is achieved through Numba-accelerated k-mer counting (17.5 ms per 5 Mb genome) and ONNX Runtime inference (0.18 ms per sample at batch 1,024). Peak memory usage is only 0.66 GB, compared to 18.76 GB for CheckM2 and 15.93 GB for CoCoPyE—a 28× reduction. For processing 100,000 genomes, MAGICC would require approximately 69 minutes on a single thread, compared to approximately 87 hours for CheckM2 (32 threads).

## Discussion

We have presented MAGICC, a tool that addresses two critical limitations of existing MAG and SAG quality assessment methods: inaccurate contamination estimation and prohibitive computational cost. By replacing functional annotation with core gene k-mer profiling, MAGICC achieves a 6.4-fold reduction in contamination MAE relative to CheckM2 while processing genomes ~1,700× faster with 28× less memory.

The key insight underlying MAGICC is that contamination fundamentally alters the nucleotide composition of a genome, and this compositional shift is readily captured by k-mer profiles. Existing tools that rely on functional annotations—whether KEGG orthologs (CheckM2, DeepCheck) or Pfam domains (CoCoPyE)—operate in a feature space where taxonomically distant organisms may appear similar, because function is more conserved than composition across the tree of life. K-mer profiles, by contrast, carry inherent taxonomic signal: organisms within the same phylum share more similar k-mer distributions than organisms across phyla, a principle well-established in metagenomic binning^12,13^ and taxonomic classification^15^. When a contaminant genome from a different phylum is introduced, it brings a distinct nucleotide composition—reflected in its k-mer profile—regardless of whether its genes encode functions already present in the host. For example, a Pseudomonadota genome contaminated with Bacillota sequences exhibits k-mers characteristic of the contaminant's own GC content and codon usage, shifting the aggregate k-mer profile in ways that CheckM2's KEGG-based features cannot detect: because KEGG orthologs are defined by function rather than taxonomic origin, a contaminant gene encoding a broadly conserved function (e.g., ribosomal protein, DNA polymerase) produces the same feature value as the native gene, and the model must rely on detecting *excess* functions—a much weaker signal than the compositional shift captured by k-mers.

The choice of 9-mers from core genes rather than whole-genome k-mers was deliberate. Core genes are present across virtually all bacteria and archaea, providing a universal feature space that does not require prior knowledge of the query genome's taxonomy. By selecting the 9,249 most prevalent k-mers across a phylogenetically diverse panel of 2,000 reference genomes, we ensure broad applicability while keeping the feature vector compact enough for efficient neural network inference. The use of raw counts rather than frequencies is also important: raw counts reflect both completeness (fewer counts when genome is incomplete) and contamination (additional counts from foreign k-mers), whereas frequencies are confounded by genome size. Indeed, many of the selected 9-mers correspond to conserved protein-coding motifs: the most prevalent 9-mer in our set, GAAGAAGAA (present in 96% of representative genomes), encodes the trinucleotide repeat for glutamic acid (Glu-Glu-Glu), found in conserved regions of single-copy core genes. When a genome is incomplete, counts of such k-mers are reduced or absent; when foreign contamination introduces additional core genes from other organisms, counts increase beyond what a single genome would produce—enabling simultaneous assessment of both quality dimensions. This compositional signal is complemented by the assembly statistics branch, which captures structural signatures of contamination such as GC bimodality, GC outlier fraction, and k-mer entropy. The cross-attention fusion mechanism enables the model to use assembly-level context (e.g., GC distribution) to modulate which k-mer patterns are most informative for a given genome.

Preliminary experiments using whole-genome k-mers instead of single-copy core gene k-mers yielded contamination MAE >20% (data not shown), far worse than the 3.60% achieved with core gene k-mers. This performance gap arises because whole-genome k-mer profiles are dominated by intergenic regions, mobile genetic elements, and repetitive sequences that are highly variable within and across lineages, introducing noise that obscures the compositional signal of contamination. By contrast, core genes provide a universal, conserved feature space that generalizes across the bacterial and archaeal tree of life, ensuring that the k-mer profile reflects stable genomic content rather than lineage-specific accessory elements. These results confirm that the taxonomic signal most relevant to quality assessment is concentrated in core gene sequences, and that restricting k-mer counting to these regions is essential for accurate prediction.

Because MAGICC's features—k-mer counts and assembly statistics—are computed directly from nucleotide sequences without reference to how the genome was obtained, MAGICC is equally applicable to single-cell amplified genomes (SAGs). SAGs suffer from amplification biases (e.g., MDA chimera formation, uneven coverage) that produce incomplete genomes with potential contamination, and the same k-mer and assembly features that detect these artifacts in MAGs are agnostic to the genome's provenance. Users can therefore apply MAGICC to SAGs without modification.

MAGICC has several limitations. First, within-phylum contamination is harder to detect than cross-phylum contamination, because closely related organisms share more similar k-mer profiles. This is an inherent limitation of any composition-based approach. Second, the model was trained on synthetic genomes, which may not capture all the complexities of real metagenomic binning artifacts, such as chimeric contigs or strain-level mixing. Third, completeness prediction accuracy (R²=0.90 across benchmarks) has room for improvement; this likely reflects the weak individual correlation of k-mer features with completeness (max |r|=0.17), suggesting that fundamentally different features (e.g., gene presence/absence) may be needed for further gains. Fourth, MAGICC currently supports bacterial and archaeal genomes; extension to other domains (e.g., viral, eukaryotic) would require domain-specific core gene sets and retraining.

Despite these limitations, MAGICC's combination of accuracy, speed, and resource efficiency makes it immediately practical for large-scale genomic surveillance, population-level microbiome studies, and real-time quality assessment pipelines. As metagenomic databases continue to grow—gcMeta now contains over 2.7 million MAGs^8^ and GTDB over 730,000 genomes^14^—the ability to assess genome quality in seconds rather than hours becomes not merely convenient but essential.

## Methods

### Reference genome curation

We downloaded metadata for all 732,475 genomes in GTDB (715,230 bacterial, 17,245 archaeal) and applied strict quality filters: CheckM2 completeness ≥98%, contamination ≤2%, contig count <100, N50 >20 kbp, and longest contig >100 kbp. This yielded 277,183 high-quality reference genomes (275,207 bacterial, 1,976 archaeal) across 110 phyla (**Table S5**). From these, 100,000 genomes were selected using square-root proportional stratified sampling across phyla to balance representation, with all available genomes included for underrepresented lineages (Patescibacteriota: 1,609; DPANN archaea: 24; candidate phyla: 586). Genomes were downloaded using the NCBI datasets CLI v18.16.0, achieving 99.96% success (99,957 of 100,000). The dataset was split into training (79,948), validation (10,010), and test (9,999) sets with stratified 80/10/10 allocation by phylum and no overlapping genomes.

### K-mer feature selection

From the training set, 1,000 representative bacterial genomes (from 97 phyla) and 1,000 representative archaeal genomes (from 13 phyla) were selected by stratified sampling. For each genome, genes were predicted using Prodigal v2.6.3 and searched against 85 bacterial core gene HMM profiles (single-copy genes present in ≥95% of bacteria) and 128 archaeal core gene HMM profiles using HMMER 3.4 with trusted cutoff thresholds. Core gene DNA sequences were extracted and canonical 9-mers were counted using KMC 3.2.2. The 9,000 most prevalent bacterial k-mers and 1,000 most prevalent archaeal k-mers were merged into a final set of 9,249 unique canonical 9-mers (751 shared between domains) (**Table S6**). The bacterial cutoff of 9,000 k-mers was chosen such that every selected k-mer is present in at least 50% of the representative bacterial genomes (minimum prevalence: 529/1,000 = 52.9%, maximum: 992/1,000). This threshold ensures broad taxonomic coverage—each selected k-mer occurs across a wide range of phyla—while keeping the feature space compact enough for efficient neural network training and inference. The archaeal k-mers (prevalence 791–998/1,000) exceeded this threshold naturally due to the higher conservation of core genes in archaeal lineages.

### Synthetic genome generation

One million synthetic genomes (800,000 training, 100,000 validation, 100,000 test) were generated (**Table S7**) with the following composition per batch of 10,000: 1,500 pure genomes (0% contamination), 1,500 complete genomes (100% completeness with varying contamination), 3,000 within-phylum contamination, 3,000 cross-phylum contamination, 500 reduced-genome organisms, and 500 archaeal genomes.

Genome fragmentation simulated four quality tiers: high (10–50 contigs, N50 100–500 kb), medium (50–200 contigs, N50 20–100 kb), low (200–500 contigs, N50 5–20 kb), and highly fragmented (500–2,000 contigs, N50 1–5 kb). Contig lengths were drawn from log-normal distributions (μ=log(N50), σ∈[0.8, 1.2]). Three bias mechanisms were applied before completeness filtering: coverage-based dropout (log-normal coverage model, threshold 5×), GC-biased loss (probability proportional to |z-score| of contig GC content), and repeat exclusion (targeting high-homopolymer contigs). Target completeness was achieved by greedily accumulating contigs from smallest to largest.

Contamination was introduced by fragmenting 1–5 contaminant genomes independently and appending their contigs. Contamination rate was defined as (total contaminant bp / dominant genome full reference length) × 100, ensuring independence from completeness. Target contamination was allocated among contaminant genomes using Dirichlet distribution. Multi-copy support allowed contamination up to 100% of the reference genome size.

### Feature extraction and normalization

For each synthetic or benchmark genome, features were extracted in two parallel streams. K-mer counts were obtained using a Numba JIT-compiled rolling hash over the genome sequence, encoding each 9-mer as a bit-packed integer and using canonical form (minimum of forward and reverse complement codes) with a lookup table for the 9,249 selected k-mers. Assembly statistics (26 features) were computed using Numba-accelerated functions for GC content, Nx/Lx metrics, and distributional properties.

Features were normalized using streaming statistics computed from the training set: k-mers were log(count+1)-transformed and Z-score standardized; length-based assembly features were log₁₀-transformed; proportion-based features were min-max scaled; and count-based features were robustly scaled using median and interquartile range from reservoir sampling.

### Neural network architecture and training

The MAGICCModelV3 architecture comprises three components (**Table S8**). The k-mer branch processes the 9,249-dimensional input through Dense(4,096)→BN→SiLU→Dropout(0.4)→SE→Dense(1,024)→BN→SiLU→Dropout(0.2)→SE→Dense(256)→BN→SiLU, where SE denotes squeeze-and-excitation attention blocks (reduction ratio 16). The assembly branch processes 26 features through Dense(128)→BN→SiLU→Dropout(0.2)→Dense(64)→BN→SiLU. The fusion head applies cross-attention (4 heads, assembly embedding as query, k-mer embedding reshaped into 16 tokens) with a gated residual connection, followed by Dense(128)→BN→SiLU→Dropout(0.1)→Dense(64)→SiLU→Dense(2). Outputs are bounded via sigmoid: completeness = σ(x)×50+50 ∈ [50,100], contamination = σ(x)×100 ∈ [0,100]. Total parameters: 44,851,010.

Training used mixed-precision FP16 with gradient checkpointing on a Quadro P2200 GPU (5.1 GB VRAM). The optimizer was AdamW (lr=1×10⁻³, weight_decay=5×10⁻⁴) with cosine annealing warm restarts (T₀=10, T_mult=2). The loss function was weighted MSE: L = 2.0×MSE(completeness) + 1.0×MSE(contamination). Data augmentation included 2% random k-mer masking and Gaussian noise injection (σ=0.01). Training ran for 84 epochs (early stopping patience=20), with best validation performance at epoch 64. The model was exported to ONNX format (opset 17, FP32) for inference.

### Benchmark evaluation

Five benchmark sets (1,000 genomes each) were generated from NCBI-finished reference genomes (Complete Genome or Chromosome assembly level) in the test split (1,810 genomes from 35 phyla): Set A (completeness gradient, 0% contamination), Set B (contamination gradient with cross-phylum contaminants, 100% completeness), Set C (1,000 Patescibacteria, uniform completeness and contamination), Set D (1,000 Archaea, uniform completeness and contamination), and Set E (mixed realistic: 200 pure + 200 complete + 600 other with 70% cross-phylum / 30% within-phylum contamination).

Competitor tools were run with default settings: CheckM2 v1.0.1 (32 threads, checkm2_py39 conda environment), CoCoPyE v0.5.0 (48 threads), and DeepCheck (PyTorch inference on CheckM2 intermediate feature vectors). Speed was normalized to genomes per minute per thread. Peak memory was measured using `/usr/bin/time -v`.

Statistical significance was assessed using the paired Wilcoxon signed-rank test (one-sided, H₁: MAGICC errors < competitor errors) with significance threshold p<0.05. Bootstrap 95% confidence intervals (1,000 resamples) were computed for all MAE estimates. MIMAG classification F1 scores were computed for Sets C and D.

### MAGICC inference pipeline

MAGICC is implemented as a Python package with a command-line interface (`python -m magicc predict`). The pipeline accepts a directory of FASTA files and produces a TSV output with predicted completeness and contamination. Multi-threaded feature extraction uses Python multiprocessing, with each worker maintaining its own Numba-compiled k-mer counter. ONNX Runtime is used for model inference. The entire pipeline—from FASTA reading to prediction output—completes in ~75 ms per genome on a single CPU thread.

## Data availability

All benchmark datasets, pre-trained models, normalization parameters, and generation scripts are available on GitHub. The genome data used for the motivating analysis (Sets A, B, C) and benchmarking (Sets A--E) have been deposited on GitHub alongside the tool. Reference genomes were obtained from the NCBI Assembly database via the NCBI datasets CLI. GTDB metadata was obtained from the Genome Taxonomy Database (https://gtdb.ecogenomic.org/).

## Code availability

MAGICC is available on GitHub ([GitHub URL]) and as a web server ([Web server URL]) under the MIT License. The package includes the pre-trained ONNX model, selected k-mer lists, normalization parameters, and a command-line interface for genome quality prediction.

## References

1. Parks, D. H. et al. Recovery of nearly 8,000 metagenome-assembled genomes substantially expands the tree of life. *Nat. Microbiol.* **2**, 1533–1542 (2017).
2. Nayfach, S. et al. A genomic catalog of Earth's microbiomes. *Nat. Biotechnol.* **39**, 499–509 (2021).
3. Bowers, R. M. et al. Minimum information about a single amplified genome (MISAG) and a metagenome-assembled genome (MIMAG) of bacteria and archaea. *Nat. Biotechnol.* **35**, 725–731 (2017).
4. Chklovski, A., Parks, D. H., Woodcroft, B. J. & Tyson, G. W. CheckM2: a rapid, scalable and accurate tool for assessing microbial genome quality using machine learning. *Nat. Methods* **20**, 1203–1212 (2023).
5. Parks, D. H., Imelfort, M., Skennerton, C. T., Hugenholtz, P. & Tyson, G. W. CheckM: assessing the quality of microbial genomes recovered from isolates, single cells, and metagenomes. *Genome Res.* **25**, 1043–1055 (2015).
6. Hyatt, D. et al. Prodigal: prokaryotic gene recognition and translation initiation site identification. *BMC Bioinformatics* **11**, 119 (2010).
7. Buchfink, B., Xie, C. & Huson, D. H. Fast and sensitive protein alignment using DIAMOND. *Nat. Methods* **12**, 59–60 (2015).
8. Cheng, R. et al. gcMeta: a Global Catalogue of Metagenomics platform to support the archiving, standardization and analysis of microbiome data. *Nucleic Acids Res.* **54**, D724–D735 (2025).
9. Özsoy, E. D. & Clean, T. CoCoPyE: fast and accurate estimation of prokaryotic genome completeness and contamination using feature engineering. *GigaScience* **13**, giae079 (2024).
10. Liao, H. & Zhang, Z. DeepCheck: multitask learning aids in assessing microbial genome quality. *Bioinformatics* **40**, btae630 (2024).
11. Orakov, A. et al. GUNC: detection of chimerism and contamination in prokaryotic genomes. *Genome Biol.* **22**, 178 (2021).
12. Kang, D. D. et al. MetaBAT 2: an adaptive binning algorithm for robust and efficient genome reconstruction from metagenome assemblies. *PeerJ* **7**, e7359 (2019).
13. Alneberg, J. et al. Binning metagenomic contigs by coverage and composition. *Nat. Methods* **11**, 1144–1146 (2014).
14. Parks, D. H. et al. GTDB: an ongoing census of bacterial and archaeal diversity through a phylogenetically consistent, rank normalized and complete genome-based taxonomy. *Nucleic Acids Res.* **50**, D199–D207 (2022).
15. Wood, D. E. & Salzberg, S. L. Kraken: ultrafast metagenomic sequence classification using exact alignments. *Genome Biol.* **15**, R46 (2014).

## Figure legends

**Figure 1. Existing tools systematically underestimate contamination from foreign organisms.** (**a**) Schematic of the three motivating dataset profiles: Set A (completeness gradient, 0% contamination), Set B (contamination gradient, 100% completeness), and Set C (realistic mixed genomes with varying completeness and contamination). (**b**) Completeness MAE for CheckM2, CoCoPyE, and DeepCheck across Sets A, B, and C. Foreign contamination in Set C significantly inflates completeness errors compared to Sets A and B. (**c**) Contamination MAE across Sets A, B, and C. All tools severely underestimate contamination in Sets B and C. (**d**) Scatter of predicted versus true contamination for Set B (contamination gradient), showing systematic underestimation by all three tools as contamination increases. (**e**) Scatter of predicted versus true contamination for Set C (realistic mixed), confirming the underestimation pattern in genomes with combined incompleteness and contamination. Only CheckM2, CoCoPyE, and DeepCheck are shown; MAGICC is excluded to present the motivating gap before introducing our solution.

**Figure 2. MAGICC workflow from data curation to training and inference.** The workflow begins with reference genome curation from GTDB (277,183 high-quality genomes across 110 phyla), followed by k-mer feature selection from core genes of 2,000 representative genomes, yielding 9,249 canonical 9-mers. One million synthetic genomes are generated with realistic fragmentation, contamination, and bias patterns. Features are extracted via two parallel streams: Numba-accelerated k-mer counting (9,249 features) and assembly statistics (26 features). These feed into a dual-branch neural network: the k-mer branch with squeeze-and-excitation attention and the assembly branch, fused via cross-attention where assembly features query k-mer tokens. Output predictions: completeness [50--100%] and contamination [0--100%]. At inference, MAGICC takes a FASTA file and produces quality predictions in ~75 ms per genome.

**Figure 3. MAGICC benchmark performance across five evaluation sets.** (**a**) Schematic of the five benchmark dataset profiles: Set A (completeness gradient), Set B (contamination gradient), Set C (Patescibacteria), Set D (Archaea), and Set E (realistic mixed). (**b**) Completeness MAE comparison across four tools (MAGICC, CheckM2, CoCoPyE, DeepCheck) for each benchmark set. (**c**) Contamination MAE comparison across four tools for each benchmark set, highlighting MAGICC's consistent advantage. (**d**) Scatter of predicted versus true contamination for Set B (contamination gradient) for all four tools. (**e**) Scatter of predicted versus true contamination for Set C (Patescibacteria). (**f**) Scatter of predicted versus true contamination for Set D (Archaea). (**g**) Scatter of predicted versus true contamination for Set E (realistic mixed genomes). Across all scatter plots, MAGICC predictions (shown in red) closely follow the diagonal, while competitor tools systematically underestimate contamination.

